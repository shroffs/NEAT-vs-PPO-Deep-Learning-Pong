{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import base64, io, time, gym\n",
    "import IPython, functools\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1c8348cbd86d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_gpu_available\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcuda_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_cuda_compute_capability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\spenc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\test_util.py\u001b[0m in \u001b[0;36mis_gpu_available\u001b[1;34m(cuda_only, min_cuda_compute_capability)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1432\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mlocal_device\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1433\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlocal_device\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"GPU\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1434\u001b[0m         if (min_cuda_compute_capability is None or\n",
      "\u001b[1;32mc:\\users\\spenc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\device_lib.py\u001b[0m in \u001b[0;36mlist_local_devices\u001b[1;34m(session_config)\u001b[0m\n\u001b[0;32m     39\u001b[0m   return [\n\u001b[0;32m     40\u001b[0m       \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m   ]\n",
      "\u001b[1;32mc:\\users\\spenc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mlist_devices\u001b[1;34m(session_config)\u001b[0m\n\u001b[0;32m   2247\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mListDevicesWithSessionConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2248\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2249\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mListDevices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found."
     ]
    }
   ],
   "source": [
    "tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment has observation space = Box(210, 160, 3)\n",
      "Number of possible actions that the agent can choose from = 6\n"
     ]
    }
   ],
   "source": [
    "# setup environment\n",
    "env = gym.make(\"Pong-v0\", frameskip=5)\n",
    "print(\"Environment has observation space =\", env.observation_space)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode observations (vertical position of left paddle, position of ball, vertical position of right paddle)\n",
    "\n",
    "def obs_preprocess(observation):\n",
    "    #take in an observation: Box(210, 160,3) *just an array\n",
    "    # return game state: int left paddle, int xball, int yball, int right paddle\n",
    "    red = observation[:,:,0]\n",
    "    red = red[35:194,:]\n",
    "    #213 is left paddle (?, 17)\n",
    "    #236 ball (?,?)\n",
    "    #92 is right paddle (?, 140)\n",
    "    try:\n",
    "        lpaddle = int(np.argwhere(red[:,17] == 213)[0]) + 9\n",
    "    except IndexError:\n",
    "        lpaddle = 0\n",
    "    try:\n",
    "        rpaddle = int(np.argwhere(red[:,140] == 92)[0]) + 9\n",
    "    except IndexError:\n",
    "        rpaddle = 0\n",
    "    \n",
    "    try:\n",
    "        xball, yball = np.argwhere(red == 92)[3]\n",
    "    except IndexError:\n",
    "        xball, yball = 0,0\n",
    "    \n",
    "    return [lpaddle, rpaddle, xball, yball]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PPO policy network\n",
    "\n",
    "class policy_network(tf.keras.Model):\n",
    "    #network takes in preprocessed obs and outputs probability of actions\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(policy_network, self).__init__()\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(8, activation=\"relu\")\n",
    "        self.fc4 = tf.keras.layers.Dense(6)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Value function netowrk\n",
    "\n",
    "class value_network(tf.keras.Model):\n",
    "    #network takes in preprocessed obs and value of given state\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(value_network, self).__init__()\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(8, activation=\"relu\")\n",
    "        self.fc4 = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create choose action function\n",
    "\n",
    "def choose_action(model, observation):\n",
    "    #takes in and model and observation\n",
    "    #outputs a single action from model output\n",
    "    \n",
    "    prob_logits = model(observation)\n",
    "    # sample actions base on probablities \n",
    "    action = tf.random.categorical(prob_logits, 1)[0]\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create memory for agent\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "        \n",
    "    def clear(self):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def add_memory(self, action, observation, reward):\n",
    "        self.actions.append(action)\n",
    "        self.observations.append(observation)\n",
    "        self.rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create discount reward function\n",
    "\n",
    "def normalize(x):\n",
    "    x -= torch.mean(x)\n",
    "    x /= torch.std(x)\n",
    "    return x\n",
    "\n",
    "def discount_reward(rewards, gamma = 0.99):\n",
    "    #input rewards: list of reward at each time step, gamma: decay factor\n",
    "    \n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    \n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        #reset reward when game ends\n",
    "        if rewards[t] != 0:\n",
    "            R = 0\n",
    "        R = R * gamma + rewards[t]\n",
    "        discounted_rewards[t] = R\n",
    "        \n",
    "    return normalize(discounted_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Advantage Function\n",
    "\n",
    "def advantage(R, observation, value_model):\n",
    "    # return list of advantages of an action given observation\n",
    "    A = R - value_model(observation)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objective function\n",
    "\n",
    "def g(epsilon, A):\n",
    "    if A >= 0:\n",
    "        return (1+epsilon)*A\n",
    "    else:\n",
    "        return (1-epsilon)*A\n",
    "\n",
    "def objective_func(policy, policy_old, advantage, action, observation, epsilon=0.2,):\n",
    "    policy_ratio = policy(observation)[action]/policy_old(observation)[action]\n",
    "    clip_objective = g(epsilon, advantage)\n",
    "    return min(policy_ratio*advantage, clip_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create value loss\n",
    "\n",
    "def value_loss(value_model, observation, reward):\n",
    "    return (value_model(observation)-reward)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss function for a batch of trajectories\n",
    "\n",
    "def loss_fn(traj, R_hat, policy_net, policy_old, value_net):\n",
    "    L_traj = []\n",
    "    L_traj_value = []\n",
    "    for k in range(len(traj)): \n",
    "        #compute loss for each time step in trajectory\n",
    "        L_time = []\n",
    "        L_time_value = []\n",
    "        \n",
    "        for t in range(len(traj[k].observations)):\n",
    "            \n",
    "            observation = traj[k].observations[t]\n",
    "            action = traj[k].actions[t]\n",
    "            adv = advantage(R_hat[k], observation[t], value_net)\n",
    "            \n",
    "            L_time[t] = objective_func(policy_net, policy_old, adv, action, observation, 0.2)\n",
    "            L_time_value[t] = value_loss(value_net, observation, R_hat[k])\n",
    "            \n",
    "        #average time step losses\n",
    "        L_traj[k] = 1/len(traj[k].observations) * np.sum(L_time)\n",
    "        L_traj_value[k] = 1/len(traj[k].observations) * np.sum(L_time_value)\n",
    "        \n",
    "    #average trajectory losses\n",
    "    L_policy = -1/len(traj) * np.sum(L_traj)\n",
    "    L_value = 1/len(traj) * np.sum(L_traj_value)\n",
    "    \n",
    "    return L_policy, L_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-2a2514b99e18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# do foward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# list of memories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtraj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\spenc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, persistent, watch_accessed_variables)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_eagerly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 798\u001b[1;33m       \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m       \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\spenc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1582\u001b[0m   \u001b[1;34m\"\"\"Initialize the context.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1583\u001b[1;33m   \u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\spenc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_is_async\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mASYNC\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m           \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFE_ContextOptionsSetAsync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFE_NewContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFE_DeleteContextOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found."
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "lr_p = 1e-4\n",
    "lr_v = 1e-4\n",
    "max_iter = 1\n",
    "batch_size = 1\n",
    "\n",
    "policy_net = policy_network()\n",
    "policy_old = copy(policy_net)\n",
    "value_net = value_network()\n",
    "policy_optimizer = tf.keras.optimizers.Adam(learning_rate = lr_p)\n",
    "value_optimizer = tf.keras.optimizers.SGD(learning_rate = lr_v)\n",
    "\n",
    "for i_iter in range(max_iter):\n",
    "    \n",
    "    # do foward pass\n",
    "    with tf.GradientTape() as tape:\n",
    "        # list of memories\n",
    "        traj = []\n",
    "        # list of summed discounted rewards from memories\n",
    "        R_hat = []\n",
    "\n",
    "        memory = Memory()\n",
    "        for i_batch in range(batch_size):\n",
    "            #reset envirnoment\n",
    "            observation = env.reset()\n",
    "\n",
    "            while True:\n",
    "                # get action\n",
    "                observation = obs_preprocess(observation)\n",
    "                action = choose_action(policy_net, observation)\n",
    "                # get next observation\n",
    "                next_observation, reward, done, info = env.step(action)\n",
    "                # add experience to memory\n",
    "                memory.add_memory(action, observation, reward)\n",
    "\n",
    "                # if play through is over\n",
    "                if done:\n",
    "                    # append memory to trajectories \n",
    "                    traj.append(memory)\n",
    "                    R_hat.append(torch.sum(discount_reward(memory.rewards)))\n",
    "                    break\n",
    "                observation = next_observation\n",
    "\n",
    "        L_policy , L_value = loss_fn(traj, R_hat, policy_net, policy_old, value_net)\n",
    "        \n",
    "    #make backwards pass\n",
    "    policy_old = copy(policy_net)\n",
    "    \n",
    "    policy_grads = tape.gradient(L_policy, policy_net.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, policy_net.trainable_variables))\n",
    "    \n",
    "    value_grads = tape.gradient(L_value, value_net.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, value_net.trainable_variables))\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
