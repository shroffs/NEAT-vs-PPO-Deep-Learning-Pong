{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import base64, io, time, gym\n",
    "import IPython, functools\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment has observation space = Box(210, 160, 3)\n",
      "Number of possible actions that the agent can choose from = 6\n"
     ]
    }
   ],
   "source": [
    "# setup environment\n",
    "env = gym.make(\"Pong-v0\", frameskip=5)\n",
    "print(\"Environment has observation space =\", env.observation_space)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode observations (vertical position of left paddle, position of ball, vertical position of right paddle)\n",
    "\n",
    "def obs_preprocess(observation):\n",
    "    #take in an observation: Box(210, 160,3) *just an array\n",
    "    # return game state: int left paddle, int xball, int yball, int right paddle\n",
    "    red = observation[:,:,0]\n",
    "    red = red[35:194,:]\n",
    "    #213 is left paddle (?, 17)\n",
    "    #236 ball (?,?)\n",
    "    #92 is right paddle (?, 140)\n",
    "    try:\n",
    "        lpaddle = int(np.argwhere(red[:,17] == 213)[0]) + 9\n",
    "    except IndexError:\n",
    "        lpaddle = 0\n",
    "    try:\n",
    "        rpaddle = int(np.argwhere(red[:,140] == 92)[0]) + 9\n",
    "    except IndexError:\n",
    "        rpaddle = 0\n",
    "    \n",
    "    try:\n",
    "        xball, yball = np.argwhere(red == 92)[3]\n",
    "    except IndexError:\n",
    "        xball, yball = 0,0\n",
    "    \n",
    "    return np.expand_dims(np.array([lpaddle, rpaddle, xball, yball], dtype=np.float32), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PPO policy network\n",
    "\n",
    "class policy_network(tf.keras.Model):\n",
    "    #network takes in preprocessed obs and outputs probability of actions\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(policy_network, self).__init__()\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(8, activation=\"relu\")\n",
    "        self.fc4 = tf.keras.layers.Dense(6)\n",
    "        self.softmax = tf.keras.layers.Softmax(-1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Value function netowrk\n",
    "\n",
    "class value_network(tf.keras.Model):\n",
    "    #network takes in preprocessed obs and value of given state\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(value_network, self).__init__()\n",
    "        \n",
    "        self.fc5 = tf.keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.fc6 = tf.keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.fc7 = tf.keras.layers.Dense(8, activation=\"relu\")\n",
    "        self.fc8 = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.fc8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create choose action function\n",
    "\n",
    "def choose_action(model, observation):\n",
    "    #takes in and model and observation\n",
    "    #outputs a single action from model output\n",
    "    \n",
    "    prob_weights = model.predict(observation)\n",
    "\n",
    "    action = np.random.choice(range(env.action_space.n), size=1, p=np.squeeze(prob_weights))[0]\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create memory for agent\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "        \n",
    "    def clear(self):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def add_memory(self, action, observation, reward):\n",
    "        self.actions.append(action)\n",
    "        self.observations.append(observation)\n",
    "        self.rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create discount reward function\n",
    "\n",
    "def normalize(x):\n",
    "    x -= tf.math.reduce_mean(x)\n",
    "    x /= tf.math.reduce_std(x)\n",
    "    return x\n",
    "\n",
    "def discount_reward(rewards, gamma = 0.99):\n",
    "    #input rewards: list of reward at each time step, gamma: decay factor\n",
    "    \n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    \n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        #reset reward when game ends\n",
    "        if rewards[t] != 0:\n",
    "            R = 0\n",
    "        R = R * gamma + rewards[t]\n",
    "        discounted_rewards[t] = R\n",
    "        \n",
    "    return normalize(discounted_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Advantage Function\n",
    "\n",
    "def advantage(R, observation, value_model):\n",
    "    # return list of advantages of an action given observation\n",
    "    A = R - value_model.predict(observation)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objective function\n",
    "\n",
    "@tf.function\n",
    "def objective_func(policy_logits, policy_old, advantage, action, observation, epsilon=0.2):\n",
    "    advantage = tf.cast(advantage, tf.float32)\n",
    "    hot_action = tf.squeeze(tf.one_hot([action],env.action_space.n))\n",
    "    policy_logits = tf.squeeze(policy_logits)\n",
    "    policy_ratio = tf.tensordot(policy_logits, hot_action, 1)/tf.tensordot(policy_old(observation), hot_action, 1)\n",
    "    clip_ratio = tf.clip_by_value(policy_ratio, 1-epsilon, 1+epsilon)\n",
    "    raw_obj = tf.math.multiply(policy_ratio, advantage)\n",
    "    clip_obj = tf.math.multiply(clip_ratio, advantage)\n",
    "    return tf.math.minimum(raw_obj, clip_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create value loss\n",
    "\n",
    "@tf.function\n",
    "def value_loss(value_logits, reward):\n",
    "    reward = tf.cast(reward, tf.float32)\n",
    "    return tf.math.pow(value_logits-reward, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that added a list of grads\n",
    "\n",
    "def add_grads(grads):\n",
    "    \n",
    "    # (n, layers, variable)\n",
    "    grads = np.transpose(np.asarray(grads)).tolist()\n",
    "    # (layers, n, variable)\n",
    "    sum_grads = []\n",
    "    for layer in grads:\n",
    "        # add grads of each layer to get one grad per layer\n",
    "        sum_grads.append(tf.math.add_n(layer))\n",
    "    # (layers, varible)\n",
    "    return sum_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss function for a batch of trajectories\n",
    "\n",
    "def get_grads(R_hat, policy_net, policy_old, value_net, observations, actions):\n",
    "    \n",
    "    #create grad storage\n",
    "    policy_grads = []\n",
    "    value_grads = []\n",
    "    \n",
    "    for observation, action in zip(observations, actions):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            #compute advantage\n",
    "            adv = advantage(R_hat, observation, value_net)\n",
    "            #get network outputs\n",
    "            policy_logits = policy_net(observation)\n",
    "            value_logits = value_net(observation)\n",
    "            #compute losses  \n",
    "            L = tf.squeeze(objective_func(policy_logits, policy_old, adv, action, observation, epsilon=0.2), 0)\n",
    "            L_value = tf.squeeze(value_loss(value_logits, R_hat), 0)\n",
    "\n",
    "        #compute gradients\n",
    "        policy_grad = tape.gradient(L, policy_net.trainable_variables)\n",
    "        value_grad = tape.gradient(L_value, value_net.trainable_variables)\n",
    "        \n",
    "        #store grads\n",
    "        policy_grads.append(policy_grad)\n",
    "        value_grads.append(value_grad)\n",
    "        \n",
    "    policy_grad = [x * 1/len(policy_grads) for x in add_grads(policy_grads)] \n",
    "    value_grad = [x * 1/len(value_grads) for x in add_grads(value_grads)] \n",
    "    \n",
    "    return policy_grad, value_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training Iteration:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batch Iteration:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batch Iteration:  20%|██        | 1/5 [02:44<10:58, 164.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batch Iteration:  40%|████      | 2/5 [03:12<06:10, 123.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batch Iteration:  60%|██████    | 3/5 [05:29<04:15, 127.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batch Iteration:  80%|████████  | 4/5 [06:00<01:38, 98.53s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-f424d18d31a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_stream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m#make forward and get gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mpolicy_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0;31m#clear memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-8dab6cbc9292>\u001b[0m in \u001b[0;36mget_grads\u001b[0;34m(R_hat, policy_net, policy_old, value_net, observations, actions)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mpolicy_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mvalue_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_BiasAddGrad\u001b[0;34m(op, received_grad)\u001b[0m\n\u001b[1;32m    348\u001b[0m   return (received_grad,\n\u001b[1;32m    349\u001b[0m           gen_nn_ops.bias_add_grad(\n\u001b[0;32m--> 350\u001b[0;31m               out_backprop=received_grad, data_format=data_format))\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add_grad\u001b[0;34m(out_backprop, data_format, name)\u001b[0m\n\u001b[1;32m    845\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m    846\u001b[0m         \u001b[0;34m\"BiasAddGrad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                        name=name)\n\u001b[0m\u001b[1;32m    848\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    735\u001b[0m           \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_MakeBool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m           \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MakeType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"list(type)\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m           attr_value.list.type.extend(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_MakeType\u001b[0;34m(v, attr_def)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     (attr_def.name, repr(v)))\n\u001b[1;32m    178\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m   \u001b[0m_SatisfiesTypeConstraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_SatisfiesTypeConstraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allowed_values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mallowed_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "lr_p = 1e-4\n",
    "lr_v = 1e-4\n",
    "max_iter = 10\n",
    "batch_size = 5\n",
    "\n",
    "policy_net = policy_network()\n",
    "policy_old = copy(policy_net)\n",
    "value_net = value_network()\n",
    "policy_optimizer = tf.keras.optimizers.Adam(learning_rate = lr_p)\n",
    "value_optimizer = tf.keras.optimizers.SGD(learning_rate = lr_v)\n",
    "memory = Memory()\n",
    "\n",
    "for i_iter in trange(max_iter, desc=\"Training Iteration\"):\n",
    "    for i_batch in trange(batch_size, desc=\"Batch Iteration\"):\n",
    "        #create grad storage\n",
    "        policy_grads = []\n",
    "        value_grads = []\n",
    "        \n",
    "        #reset envirnoment\n",
    "        observation = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # get action\n",
    "            observation = obs_preprocess(observation)\n",
    "            action = choose_action(policy_net, observation)\n",
    "            # get next observation\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            # add experience to memory\n",
    "            memory.add_memory(action, observation, reward)\n",
    "\n",
    "            # if play through is over\n",
    "            if done:\n",
    "                #compute discounted reward of run\n",
    "                R_hat = tf.math.reduce_sum(discount_reward(memory.rewards))\n",
    "                #make forward and get gradients\n",
    "                policy_grad, value_grad = get_grads(R_hat, policy_net, policy_old, value_net, memory.observations, memory.actions)\n",
    "                #clear memory\n",
    "                memory.clear()\n",
    "                break\n",
    "                \n",
    "            observation = next_observation\n",
    "            \n",
    "    #sum and average grads of trajectories\n",
    "    policy_grad = [x * 1/len(policy_grads) for x in add_grads(policy_grads)] \n",
    "    value_grad = [x * 1/len(value_grads) for x in add_grads(value_grads)] \n",
    "            \n",
    "    #copy old policy        \n",
    "    policy_old = copy(policy_net)\n",
    "    \n",
    "    #make backwards prop and step\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, policy_net.trainable_variables))\n",
    "    value_optimizer.apply_gradients(zip(value_grads, value_net.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
