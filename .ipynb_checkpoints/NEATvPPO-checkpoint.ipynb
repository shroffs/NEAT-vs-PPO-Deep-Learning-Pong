{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import base64, io, time, gym\n",
    "import IPython, functools\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment has observation space = Box(210, 160, 3)\n",
      "Number of possible actions that the agent can choose from = 6\n"
     ]
    }
   ],
   "source": [
    "# setup environment\n",
    "env = gym.make(\"Pong-v0\", frameskip=5)\n",
    "print(\"Environment has observation space =\", env.observation_space)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode observations (vertical position of left paddle, position of ball, vertical position of right paddle)\n",
    "\n",
    "def obs_preprocess(observation):\n",
    "    #take in an observation: Box(210, 160,3) *just an array\n",
    "    # return game state: int left paddle, int xball, int yball, int right paddle\n",
    "    red = observation[:,:,0]\n",
    "    red = red[35:194,:]\n",
    "    #213 is left paddle (?, 17)\n",
    "    #236 ball (?,?)\n",
    "    #92 is right paddle (?, 140)\n",
    "    try:\n",
    "        lpaddle = int(np.argwhere(red[:,17] == 213)[0]) + 9\n",
    "    except IndexError:\n",
    "        lpaddle = 0\n",
    "    try:\n",
    "        rpaddle = int(np.argwhere(red[:,140] == 92)[0]) + 9\n",
    "    except IndexError:\n",
    "        rpaddle = 0\n",
    "    \n",
    "    try:\n",
    "        xball, yball = np.argwhere(red == 92)[3]\n",
    "    except IndexError:\n",
    "        xball, yball = 0,0\n",
    "    \n",
    "    return np.expand_dims(np.array([lpaddle, rpaddle, xball, yball], dtype=np.float32), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PPO policy network\n",
    "\n",
    "class policy_network(tf.keras.Model):\n",
    "    #network takes in preprocessed obs and outputs probability of actions\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(policy_network, self).__init__()\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(8, activation=\"relu\")\n",
    "        self.fc4 = tf.keras.layers.Dense(6)\n",
    "        self.softmax = tf.keras.layers.Softmax(-1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Value function netowrk\n",
    "\n",
    "class value_network(tf.keras.Model):\n",
    "    #network takes in preprocessed obs and value of given state\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(value_network, self).__init__()\n",
    "        \n",
    "        self.fc5 = tf.keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.fc6 = tf.keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.fc7 = tf.keras.layers.Dense(8, activation=\"relu\")\n",
    "        self.fc8 = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.fc8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create choose action function\n",
    "\n",
    "def choose_action(model, observation):\n",
    "    #takes in and model and observation\n",
    "    #outputs a single action from model output\n",
    "    \n",
    "    prob_weights = model.predict(observation)\n",
    "\n",
    "    action = np.random.choice(range(env.action_space.n), size=1, p=np.squeeze(prob_weights))[0]\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create memory for agent\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "        \n",
    "    def clear(self):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def add_memory(self, action, observation, reward):\n",
    "        self.actions.append(action)\n",
    "        self.observations.append(observation)\n",
    "        self.rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create discount reward function\n",
    "\n",
    "def normalize(x):\n",
    "    x -= tf.math.reduce_mean(x)\n",
    "    x /= tf.math.reduce_std(x)\n",
    "    return x\n",
    "\n",
    "def discount_reward(rewards, gamma = 0.99):\n",
    "    #input rewards: list of reward at each time step, gamma: decay factor\n",
    "    \n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    \n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        #reset reward when game ends\n",
    "        if rewards[t] != 0:\n",
    "            R = 0\n",
    "        R = R * gamma + rewards[t]\n",
    "        discounted_rewards[t] = R\n",
    "        \n",
    "    return normalize(discounted_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Advantage Function\n",
    "\n",
    "def advantage(R, observation, value_model):\n",
    "    # return list of advantages of an action given observation\n",
    "    A = R - value_model.predict(observation)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objective function\n",
    "\n",
    "def objective_func(policy, policy_old, advantage, action, observation, epsilon=0.2):\n",
    "    advantage = tf.cast(advantage, tf.float32)\n",
    "    policy_ratio = policy.predict(observation)[0,action]/policy_old.predict(observation)[0,action]\n",
    "    clip_ratio = tf.clip_by_value(policy_ratio, 1-epsilon, 1+epsilon)\n",
    "    raw_obj = tf.math.multiply(policy_ratio, advantage)\n",
    "    clip_obj = tf.math.multiply(clip_ratio, advantage)\n",
    "    return tf.math.minimum(raw_obj, clip_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create value loss\n",
    "\n",
    "def value_loss(value_model, observation, reward):\n",
    "    reward = tf.cast(reward, tf.float32)\n",
    "    return tf.math.pow(value_model.predict(observation)-reward, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'value_network_79/dense_636/kernel:0' shape=(4, 16) dtype=float32>, <tf.Variable 'value_network_79/dense_636/bias:0' shape=(16,) dtype=float32>, <tf.Variable 'value_network_79/dense_637/kernel:0' shape=(16, 16) dtype=float32>, <tf.Variable 'value_network_79/dense_637/bias:0' shape=(16,) dtype=float32>, <tf.Variable 'value_network_79/dense_638/kernel:0' shape=(16, 8) dtype=float32>, <tf.Variable 'value_network_79/dense_638/bias:0' shape=(8,) dtype=float32>, <tf.Variable 'value_network_79/dense_639/kernel:0' shape=(8, 1) dtype=float32>, <tf.Variable 'value_network_79/dense_639/bias:0' shape=(1,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(value_net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss function for a batch of trajectories\n",
    "\n",
    "def training_step(traj, R_hat, policy_net, policy_old, value_net):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        #get losses\n",
    "        L_traj = tf.constant([], dtype=tf.float32)\n",
    "        L_traj_value = tf.constant([], dtype=tf.float32)\n",
    "\n",
    "        for k in range(len(traj)): \n",
    "            #compute loss for each time step in trajectory\n",
    "            L_time = tf.constant([], dtype=tf.float32)\n",
    "            L_time_value = tf.constant([], dtype=tf.float32)\n",
    "\n",
    "            for t in range(len(traj[k].observations)):\n",
    "\n",
    "                observation = traj[k].observations[t]\n",
    "                action = traj[k].actions[t]\n",
    "                adv = advantage(R_hat[k], observation, value_net)\n",
    "                \n",
    "                L_t = tf.squeeze(objective_func(policy_net, policy_old, adv, action, observation, epsilon=0.2), 0)\n",
    "                L_t_value = tf.squeeze(value_loss(value_net, observation, R_hat[k]), 0)\n",
    "\n",
    "                L_time = tf.concat([L_time, L_t],0)\n",
    "                L_time_value = tf.concat([L_time_value, L_t_value],0)\n",
    "\n",
    "            #average time step losses\n",
    "            L_t_avg = tf.math.reduce_mean(L_time, keepdims=True)\n",
    "            L_t_avg_value = tf.math.reduce_mean(L_time_value, keepdims=True)\n",
    "            \n",
    "            L_traj = tf.concat([L_traj, L_t_avg],0)\n",
    "            L_traj_value = tf.concat([L_traj_value, L_t_avg_value],0)\n",
    "\n",
    "        #average trajectory losses\n",
    "        L_policy = -1*tf.math.reduce_sum(L_traj, keepdims=True)\n",
    "        L_value = tf.math.reduce_sum(L_traj_value, keepdims=True)\n",
    "        \n",
    "    #make backwards pass\n",
    "    policy_grads = tape.gradient(L_policy, policy_net.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, policy_net.trainable_variables))\n",
    "    \n",
    "    value_grads = tape.gradient(L_value, value_net.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, value_net.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method policy_network.call of <__main__.policy_network object at 0x7fe5f4da19b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method policy_network.call of <__main__.policy_network object at 0x7fe5f4da19b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method policy_network.call of <__main__.policy_network object at 0x7fe5f4da19b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method policy_network.call of <__main__.policy_network object at 0x7fe5f4da19b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method value_network.call of <__main__.value_network object at 0x7fe5e84fb438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method value_network.call of <__main__.value_network object at 0x7fe5e84fb438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method value_network.call of <__main__.value_network object at 0x7fe5e84fb438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method value_network.call of <__main__.value_network object at 0x7fe5e84fb438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "[None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['policy_network_81/dense_648/kernel:0', 'policy_network_81/dense_648/bias:0', 'policy_network_81/dense_649/kernel:0', 'policy_network_81/dense_649/bias:0', 'policy_network_81/dense_650/kernel:0', 'policy_network_81/dense_650/bias:0', 'policy_network_81/dense_651/kernel:0', 'policy_network_81/dense_651/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-338-1b1ec32bd934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#make backwards prop and step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-337-c1751c06bc89>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(traj, R_hat, policy_net, policy_old, value_net)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpolicy_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mvalue_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m    973\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m--> 975\u001b[0;31m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[1;32m    976\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m     logging.warning(\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['policy_network_81/dense_648/kernel:0', 'policy_network_81/dense_648/bias:0', 'policy_network_81/dense_649/kernel:0', 'policy_network_81/dense_649/bias:0', 'policy_network_81/dense_650/kernel:0', 'policy_network_81/dense_650/bias:0', 'policy_network_81/dense_651/kernel:0', 'policy_network_81/dense_651/bias:0']."
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "lr_p = 1e-4\n",
    "lr_v = 1e-4\n",
    "max_iter = 1\n",
    "batch_size = 1\n",
    "\n",
    "policy_net = policy_network()\n",
    "policy_old = copy(policy_net)\n",
    "value_net = value_network()\n",
    "policy_optimizer = tf.keras.optimizers.Adam(learning_rate = lr_p)\n",
    "value_optimizer = tf.keras.optimizers.SGD(learning_rate = lr_v)\n",
    "\n",
    "for i_iter in range(max_iter):\n",
    "    \n",
    "    # collect trajectories and rewards\n",
    "    # list of memories\n",
    "    traj = []\n",
    "    # list of summed discounted rewards from memories\n",
    "    R_hat = []\n",
    "\n",
    "    memory = Memory()\n",
    "    for i_batch in range(batch_size):\n",
    "        #reset envirnoment\n",
    "        observation = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # get action\n",
    "            observation = obs_preprocess(observation)\n",
    "            action = choose_action(policy_net, observation)\n",
    "            # get next observation\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            # add experience to memory\n",
    "            memory.add_memory(action, observation, reward)\n",
    "\n",
    "            # if play through is over\n",
    "            if done:\n",
    "                # append memory to trajectories \n",
    "                traj.append(memory)\n",
    "                R_hat.append(tf.math.reduce_sum(discount_reward(memory.rewards)))\n",
    "                break\n",
    "            observation = next_observation\n",
    "            \n",
    "    #copy old policy        \n",
    "    policy_old = copy(policy_net)\n",
    "    \n",
    "    #make backwards prop and step\n",
    "    training_step(traj, R_hat, policy_net, policy_old, value_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
